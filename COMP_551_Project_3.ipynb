{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP 551 Project 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM4D06wxqHiV"
      },
      "source": [
        "# **Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtEG1DdY0eCr"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount(\"/content/gdrive\")\r\n",
        "\r\n",
        "import h5py\r\n",
        "import numpy as np \r\n",
        "import cv2\r\n",
        "\r\n",
        "# load all datasets from google drive\r\n",
        "filepath = \"/content/gdrive/My Drive/551-P3/MNIST_synthetic.h5\"\r\n",
        "h5 = h5py.File(filepath, 'r')\r\n",
        "test_dataset = h5['test_dataset']\r\n",
        "train_dataset = h5['train_dataset']\r\n",
        "train_labels = h5['train_labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1BVfZSLzm5L"
      },
      "source": [
        "# **Apply Image Segmentation to One Image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u9pw4hkbfPJ"
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np \n",
        "\n",
        "# this function returns all segmented images in a list\n",
        "def getPics(image, mode):\n",
        "\n",
        "    # start by resizing the image\n",
        "    image = cv2.resize(image, (320, 320)) \n",
        "\n",
        "    # apply threshhold\n",
        "    thresh = cv2.threshold(image,0,255,cv2.THRESH_BINARY)[1]\n",
        "    \n",
        "    # apply morphology\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\n",
        "    morph = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    # find all contours\n",
        "    counts = cv2.findContours(morph,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) \n",
        "\n",
        "    # get average area of each contour for later filtering\n",
        "    meanCountArea = np.mean([cv2.contourArea(k) for k in cnts[0]])\n",
        "\n",
        "    images = []\n",
        "    countour_boxes = []\n",
        "\n",
        "    # build boxes\n",
        "    for (i,contour) in enumerate(counts[0]):\n",
        "        if cv2.contourArea(c) < meanCountArea/2.4:\n",
        "            continue\n",
        "        (x,y,w,h) = cv2.boundingRect(contour)\n",
        "        countour_boxes.append((x,y,w,h))\n",
        "\n",
        "    # crop image and append to list\n",
        "    for (x,y,w,h) in sorted(countour_boxes):\n",
        "        # add padding to the bounding box to make image less pixelated\n",
        "        crop_img = image[y-8:y+h+8, x-8:x+w+8]\n",
        "\n",
        "        # resize the image to 28, 28 and change dimensions\n",
        "        resized = cv2.copyMakeBorder(crop_img, 5, 5, 5, 5, cv2.BORDER_CONSTANT, 255)\n",
        "        resized = cv2.resize(resized, (28, 28))\n",
        "        crop_img = np.reshape(resized,(28,28,1))\n",
        "        images.append(crop_img)\n",
        "        \n",
        "    if mode == 0: return len(images)\n",
        "    else: return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcwERmAi6bxl"
      },
      "source": [
        "# **Apply Image Segmentation to Multiple Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWqqkVOu6hPn"
      },
      "source": [
        "# this loops through all images in the dataset to apply segmentation to each image one at a time\r\n",
        "main = []\r\n",
        "for i in range(len(test_dataset)):\r\n",
        "  # print(i)\r\n",
        "  images = getPics(test_dataset[i], 1)\r\n",
        "  main.append(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3OwsWIM61qk"
      },
      "source": [
        "\r\n",
        "# **Benchmark Performance of Image Segmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN-FptKy7BAJ"
      },
      "source": [
        "# this benchmark compares the length of the train sample and the amount of segmented digits to determine whether all digits were successfully segmentated\r\n",
        "\r\n",
        "total_error_count = 0\r\n",
        "under_error_count = 0\r\n",
        "over_error_count = 0\r\n",
        "images = []\r\n",
        "\r\n",
        "# loop through all images in dataset\r\n",
        "for i in range(len(train_dataset)):\r\n",
        "  actual = train_labels[i]\r\n",
        "  # remove 10's to have accurate length\r\n",
        "  actual = [x for x in actual if x != 10]\r\n",
        "  image = train_dataset[i]\r\n",
        "\r\n",
        "  result = getPics(image, 0)\r\n",
        "  actual = train_labels[i]\r\n",
        "\r\n",
        "  if result > actual: over_error_count += 1\r\n",
        "  if result < actual: under_error_count += 1\r\n",
        "  if result != actual: total_error_count += 1\r\n",
        "\r\n",
        "print(\"Accuracy of amount of digits in picture\\nOut of\", i + 1, \"images, you got\", total_error_count, \"wrong\\n\")\r\n",
        "print((i - total_error_count)/i*100, \"% accuracy!\")\r\n",
        "print(\"under_count\", under_error_count)\r\n",
        "print(\"over_count\", over_error_count)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsjJ4YuV7zW6"
      },
      "source": [
        "# **Train/Validation on Original Provided Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCPgAG2k73nk"
      },
      "source": [
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import h5py\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, Dense, Flatten, BatchNormalization\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def getPics(image):\r\n",
        "\r\n",
        "    # start by resizing the image\r\n",
        "    image = cv2.resize(image, (320, 320)) \r\n",
        "\r\n",
        "    # apply threshhold\r\n",
        "    thresh = cv2.threshold(image,0,255,cv2.THRESH_BINARY)[1]\r\n",
        "    \r\n",
        "    # apply morphology\r\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))\r\n",
        "    morph = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\r\n",
        "\r\n",
        "    # find all contours\r\n",
        "    counts = cv2.findContours(morph,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) \r\n",
        "\r\n",
        "    # get average area of each contour for later filtering\r\n",
        "    meanCountArea = np.mean([cv2.contourArea(k) for k in cnts[0]])\r\n",
        "\r\n",
        "    images = []\r\n",
        "    countour_boxes = []\r\n",
        "\r\n",
        "    # build boxes\r\n",
        "    for (i,contour) in enumerate(counts[0]):\r\n",
        "        if cv2.contourArea(c) < meanCountArea/2.4:\r\n",
        "            continue\r\n",
        "        (x,y,w,h) = cv2.boundingRect(contour)\r\n",
        "        countour_boxes.append((x,y,w,h))\r\n",
        "\r\n",
        "    # crop image and append to list\r\n",
        "    for (x,y,w,h) in sorted(countour_boxes):\r\n",
        "        # add padding to the bounding box to make image less pixelated\r\n",
        "        crop_img = image[y-8:y+h+8, x-8:x+w+8]\r\n",
        "\r\n",
        "        # resize the image to 28, 28 and change dimensions\r\n",
        "        resized = cv2.copyMakeBorder(crop_img, 5, 5, 5, 5, cv2.BORDER_CONSTANT, 255)\r\n",
        "        resized = cv2.resize(resized, (28, 28))\r\n",
        "        crop_img = np.reshape(resized,(28,28,1))\r\n",
        "        images.append(crop_img)\r\n",
        "        \r\n",
        "    return images\r\n",
        "\r\n",
        "# import dataset\r\n",
        "filepath = \"./MNIST_synthetic.h5\"\r\n",
        "h5 = h5py.File(filepath, 'r')\r\n",
        "\r\n",
        "test_dataset = h5['test_dataset']\r\n",
        "train_dataset = h5['train_dataset']\r\n",
        "train_labels = h5['train_labels']\r\n",
        "main = []\r\n",
        "\r\n",
        "# build list that holds all images\r\n",
        "for i in range(len(test_dataset)):\r\n",
        "    images = getPics(test_dataset[i])\r\n",
        "    main.append(images)\r\n",
        "\r\n",
        "# set up model training parameters\r\n",
        "X_train = []\r\n",
        "y_train = []\r\n",
        "\r\n",
        "for i in range(len(main)):\r\n",
        "    for j in range(len(main[i])):\r\n",
        "        X_train.append(main[i][j])\r\n",
        "        y_train.append(train_labels[i][j])\r\n",
        "\r\n",
        "X_train = np.array(X_train)\r\n",
        "y_train = np.array(y_train)\r\n",
        "\r\n",
        "print(X_train.shape, y_train.shape)\r\n",
        "\r\n",
        "\r\n",
        "# set up model \r\n",
        "batch_size = 128\r\n",
        "epochs = 40\r\n",
        "num_models = 15\r\n",
        "records = [0] * num_models\r\n",
        "averages_train = [0]  * num_models\r\n",
        "averages_validation = [0]  * num_models\r\n",
        "\r\n",
        "model_names = [\"model1\", \"model2\", \"model3\", \"model4\", \"model5\", \"model6\", \"model7\", \"model8\", \"model9\", \"model10\", \"model11\", \"model12\", \"model13\", \"model14\", \"model15\"]\r\n",
        "callbacks = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=0, verbose=1, mode='min', baseline=None, restore_best_weights=True)\r\n",
        "datagen = ImageDataGenerator(rotation_range=15, zoom_range=0.15, width_shift_range=0.15,height_shift_range=0.15)\r\n",
        "accuracy = [0] * num_models\r\n",
        "model = [0] * num_models\r\n",
        "\r\n",
        "# add all model parameters\r\n",
        "for i in range(num_models):\r\n",
        "    model[i] = Sequential()\r\n",
        "    model[i].add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'))\r\n",
        "    model[i].add(BatchNormalization())\r\n",
        "    model[i].add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\r\n",
        "    model[i].add(BatchNormalization())\r\n",
        "    model[i].add(Conv2D(filters=32, kernel_size=(5, 5), strides=2, activation='relu', padding='same'))\r\n",
        "    model[i].add(BatchNormalization())\r\n",
        "    model[i].add(Dropout(rate=0.3))\r\n",
        "    model[i].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\r\n",
        "    model[i].add(BatchNormalization())\r\n",
        "    model[i].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\r\n",
        "    model[i].add(BatchNormalization())\r\n",
        "    model[i].add(Conv2D(filters=64, kernel_size=(5, 5), strides=2, activation='relu', padding='same'))\r\n",
        "    model[i].add(BatchNormalization())\r\n",
        "    model[i].add(Dropout(rate=0.3))\r\n",
        "    model[i].add(Flatten())\r\n",
        "    model[i].add(Dense(128, activation='relu'))\r\n",
        "    model[i].add(Dropout(rate=0.3))\r\n",
        "    model[i].add(Dense(10, activation='softmax'))\r\n",
        "    model[i].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "records = [[0] * epochs] * num_models\r\n",
        "\r\n",
        "# start training each model\r\n",
        "for i in range(num_models):\r\n",
        "    X_val = X_train[0: int(X_train.shape[0]/4)]\r\n",
        "    y_val = y_train[0:int(y_train.shape[0]/4)]\r\n",
        "    X_train_reduced = X_train[int(X_train.shape[0]/4):(X_train.shape[0])]\r\n",
        "    y_train_reduced = y_train[int(X_train.shape[0]/4):(X_train.shape[0])]\r\n",
        "\r\n",
        "    print(X_val.shape)\r\n",
        "    print(y_val.shape)\r\n",
        "    print(X_train_reduced.shape)\r\n",
        "    print(y_train_reduced.shape)\r\n",
        "\r\n",
        "    records[i] = model[i].fit_generator(datagen.flow(X_train_reduced, y_train_reduced, batch_size=batch_size), epochs=epochs, validation_data=(X_val, y_val), verbose = 1)\r\n",
        "    print(\" CNN \", i + 1, \" Epochs Trained: \", epochs, \" Training Accuracy: \", format(max(records[i].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i].history['val_accuracy']), \".5f\"), \" Training Loss: \", format(min(records[i].history['loss']), \".5f\"), \" Validation Loss: \", format(min(records[i].history['val_loss']), \".5f\"))\r\n",
        "\r\n",
        "# save each model\r\n",
        "for i in range(num_models):\r\n",
        "    model[i].save(model_names[i] + \"2\")\r\n",
        "\r\n",
        "# output performance statistics\r\n",
        "for i in range(num_models):\r\n",
        "    averages_train[i] = max(records[i].history['accuracy'])\r\n",
        "    averages_validation[i] = max(records[i].history['val_accuracy'])\r\n",
        "t = 0\r\n",
        "v = 0\r\n",
        "\r\n",
        "# set up train/valiation\r\n",
        "for i in range(num_models):\r\n",
        "    t += averages_train[i]\r\n",
        "    v += averages_validation[i]\r\n",
        "t = t/num_models\r\n",
        "v = v/num_models\r\n",
        "\r\n",
        "print(\"Training Accuracy\", t)\r\n",
        "print(\"Validation Accuracy\", v)\r\n",
        "\r\n",
        "at = [[0] * epochs] * num_models\r\n",
        "av = [[0] * epochs] * num_models\r\n",
        "\r\n",
        "for i in num_models:\r\n",
        "    at[i] = records[i].history['accuracy']\r\n",
        "    av[i] = records[i].history['val_accuracy']\r\n",
        "\r\n",
        "for i in range(num_models):\r\n",
        "    at[i] = [x / num_models for x in at[i]]\r\n",
        "    av[i] = [x / num_models for x in av[i]]\r\n",
        "\r\n",
        "h5.close()\r\n",
        "\r\n",
        "# plot graphs for accuracy and validation\r\n",
        "plt.plot(list(range(1,epochs+1,1)), at)\r\n",
        "plt.plot(list(range(1,epochs+1,1)), av)\r\n",
        "plt.xlabel('Epoch')\r\n",
        "plt.ylabel('Validation Accuaracy')\r\n",
        "plt.title('Accuracy of Model Over Epochs')\r\n",
        "plt.legend([\"Training Accuracy\", \"Validation Accuracy\"], loc=\"upper left\")\r\n",
        "axes = plt.gca()\r\n",
        "axes.set_ylim([0.95,1])\r\n",
        "plt.xticks(list(range(1,epochs+1,1)))\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvz_a0-9Othm"
      },
      "source": [
        "# **Build Deep Neural Network I**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "avTjT1qdMthX",
        "outputId": "7a9d16c9-b5f9-4620-f06f-79ecd2f2cdfc"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import History\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_rows, image_cols = 28, 28\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "X_train = X_train.reshape(X_train.shape[0], image_rows, image_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_rows, image_cols, 1)\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "\n",
        "#first test to see what is the best # of hidden layers between 1-4 for our model\n",
        "sizes_tested = 4\n",
        "model = [[0] * 3] * sizes_tested\n",
        "model_names = [\"1 Hidden Layer\", \"2 Hidden Layers\", \"3 Hidden Layers\", \"4 Hidden Layers\" ]\n",
        "\n",
        "#build the different models, 3 of each type so we get an average across each parameter we're testing for\n",
        "for i in range(sizes_tested):\n",
        "  for j in range(3):\n",
        "    if (i == 0):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=24, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
        "    \n",
        "    if (i == 1):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=24, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Conv2D(filters=48, kernel_size=(5, 5), activation='relu', padding='same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    if (i == 2):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(24, kernel_size=(5,5), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Conv2D(48, kernel_size=(5,5), activation='relu', padding='same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Conv2D(64, kernel_size=(5,5), padding='same', activation='relu'))\n",
        "      model[i][j].add(Conv2D(filters=48, kernel_size=(5, 5), activation='relu', padding='same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    if (i == 3):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=24, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Conv2D(filters=48, kernel_size=(5, 5), activation='relu', padding='same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu', padding='same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Conv2D(filters=96, kernel_size=(5, 5), activation='relu', padding='same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    model[i][j].add(Flatten())\n",
        "    model[i][j].add(Dense(256, activation='relu'))\n",
        "    model[i][j].add(Dense(10, activation='softmax'))\n",
        "    model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#split data into train and val sets\n",
        "X_val = X_train[0: int(X_train.shape[0]/4)]\n",
        "y_val = y_train[0:int(y_train.shape[0]/4)]\n",
        "X_train_reduced = X_train[int(X_train.shape[0]/4):(X_train.shape[0])] \n",
        "y_train_reduced = y_train[int(X_train.shape[0]/4):(X_train.shape[0])] \n",
        "records = [[0] * 3] * sizes_tested\n",
        "averages = [[0] * epochs] * sizes_tested\n",
        "\n",
        "#train it and print performance \n",
        "print(\"Start Training...\")\n",
        "for i in range(sizes_tested):\n",
        "  for j in range(3):\n",
        "    records[i][j] = model[i][j].fit(X_train_reduced,y_train_reduced, epochs = epochs, batch_size=batch_size, validation_data = (X_val,y_val), verbose=0)\n",
        "    print(\"Run Number: \", j+1, \" Number of hidden layers: \", i+1, \" Epochs Trained: \", epochs, \" Training Accuracy: \", format(max(records[i][j].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i][j].history['val_accuracy']), \".5f\"), \" Training Loss: \", format(min(records[i][j].history['loss']), \".5f\"), \" Validation Loss: \", format(min(records[i][j].history['val_loss']), \".5f\"))\n",
        "    averages[i] = list(x + y for (x,y) in zip(averages[i], records[i][j].history['val_accuracy']))\n",
        "\n",
        "#print graphs of performance of different model architecture across epochs \n",
        "for i in range(sizes_tested):\n",
        "  averages[i] = [x / 3 for x in averages[i]]\n",
        "  plt.plot(list(range(1,epochs+1,1)), averages[i])\n",
        "print(averages)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuaracy')\n",
        "plt.title('Accuracy of Model Over Epochs')\n",
        "plt.legend(model_names, loc=\"upper left\")\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.97,1])\n",
        "plt.xticks(list(range(1,epochs+1,1)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Start Training...\n",
            "Run Number:  1  Number of hidden layers:  1  Epochs Trained:  10  Training Accuracy:  0.99431  Validation Accuracy:  0.98840  Training Loss:  0.01876  Validation Loss:  0.04638\n",
            "Run Number:  2  Number of hidden layers:  1  Epochs Trained:  10  Training Accuracy:  0.99444  Validation Accuracy:  0.98780  Training Loss:  0.01803  Validation Loss:  0.04824\n",
            "Run Number:  3  Number of hidden layers:  1  Epochs Trained:  10  Training Accuracy:  0.99420  Validation Accuracy:  0.98780  Training Loss:  0.01823  Validation Loss:  0.04668\n",
            "Run Number:  1  Number of hidden layers:  2  Epochs Trained:  10  Training Accuracy:  0.99716  Validation Accuracy:  0.98987  Training Loss:  0.01093  Validation Loss:  0.04742\n",
            "Run Number:  2  Number of hidden layers:  2  Epochs Trained:  10  Training Accuracy:  0.99698  Validation Accuracy:  0.98940  Training Loss:  0.01108  Validation Loss:  0.04805\n",
            "Run Number:  3  Number of hidden layers:  2  Epochs Trained:  10  Training Accuracy:  0.99680  Validation Accuracy:  0.99020  Training Loss:  0.01136  Validation Loss:  0.04535\n",
            "Run Number:  1  Number of hidden layers:  3  Epochs Trained:  10  Training Accuracy:  0.99869  Validation Accuracy:  0.98960  Training Loss:  0.00500  Validation Loss:  0.05605\n",
            "Run Number:  2  Number of hidden layers:  3  Epochs Trained:  10  Training Accuracy:  0.99816  Validation Accuracy:  0.99073  Training Loss:  0.00638  Validation Loss:  0.06445\n",
            "Run Number:  3  Number of hidden layers:  3  Epochs Trained:  10  Training Accuracy:  0.99851  Validation Accuracy:  0.99173  Training Loss:  0.00590  Validation Loss:  0.04705\n",
            "Run Number:  1  Number of hidden layers:  4  Epochs Trained:  10  Training Accuracy:  0.99929  Validation Accuracy:  0.98993  Training Loss:  0.00336  Validation Loss:  0.06601\n",
            "Run Number:  2  Number of hidden layers:  4  Epochs Trained:  10  Training Accuracy:  0.99898  Validation Accuracy:  0.99020  Training Loss:  0.00426  Validation Loss:  0.07508\n",
            "Run Number:  3  Number of hidden layers:  4  Epochs Trained:  10  Training Accuracy:  0.99840  Validation Accuracy:  0.98987  Training Loss:  0.00658  Validation Loss:  0.06864\n",
            "[[0.9758222301801046, 0.9813555677731832, 0.9832889040311178, 0.9853555560112, 0.983133335908254, 0.9846888979276022, 0.9841111302375793, 0.9845777750015259, 0.9878888924916586, 0.986133337020874], [0.9870888988176981, 0.9866888920466105, 0.9855111042658488, 0.9866666595141093, 0.9871777693430582, 0.9889777700106303, 0.9873333175977071, 0.9884666601816813, 0.9875555634498596, 0.9889333248138428], [0.9866222341855367, 0.9890666604042053, 0.9853111306826273, 0.9891555507977804, 0.9864222009976705, 0.9855555693308512, 0.9881111184755961, 0.9885111053784689, 0.9892666737238566, 0.9884222348531088], [0.9875777761141459, 0.9876888990402222, 0.9858888983726501, 0.9888666669527689, 0.9878222346305847, 0.9884666601816813, 0.9892888863881429, 0.9883777896563212, 0.9880666534105936, 0.988955557346344]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmfNMok0MDh9"
      },
      "source": [
        "From the graph produced from the code above we can determine that a model with 2 hidden layers is best as it's accuracy is similar to 3 or 4 hidden layers, but its training time will be significantly shorter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W0sGJSiJlYh"
      },
      "source": [
        "# **Build Deep Neural Network II**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "BghYJ9E9Jtbf",
        "outputId": "b04bf80a-6783-45cb-ae0d-8413f29d1cc8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import History\n",
        "from keras.callbacks import TensorBoard\n",
        "history = History()\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_rows, image_cols = 28, 28\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "X_train = X_train.reshape(X_train.shape[0], image_rows, image_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_rows, image_cols, 1)\n",
        "batch_size = 100\n",
        "epochs = 15\n",
        "\n",
        "#next we test to see what is the best # of filters for the two layer model\n",
        "filters_tested = 5\n",
        "model = [[0] * 3] * filters_tested\n",
        "model_names = [\"16 and 32 filters\", \"24 and 48 filters\", \"32 and 64 filters\", \"48 and 96 filters\", \"64 and 128 filters\" ]\n",
        "first_layer_filter_size = [16, 24, 32, 48, 64]\n",
        "averages = [[0] * epochs] * filters_tested\n",
        "\n",
        "#build the different models, 3 of each type so we get an average across each parameter we're testing for\n",
        "for i in range(filters_tested):\n",
        "  for j in range(3):\n",
        "    model[i][j] = Sequential()\n",
        "    model[i][j].add(Conv2D(filters=first_layer_filter_size[i], kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
        "    model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model[i][j].add(Conv2D(filters=(first_layer_filter_size[i] * 2), kernel_size=(5, 5), activation='relu', padding='same'))\n",
        "    model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model[i][j].add(Flatten())\n",
        "    model[i][j].add(Dense(256, activation='relu'))\n",
        "    model[i][j].add(Dense(10, activation='softmax'))\n",
        "    model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#split data into train and val sets\n",
        "X_val = X_train[0: int(X_train.shape[0]/4)]\n",
        "y_val = y_train[0:int(y_train.shape[0]/4)]\n",
        "X_train_reduced = X_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "y_train_reduced = y_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "records = [[0] * 3] * filters_tested\n",
        "averages = [[0] * epochs] * filters_tested\n",
        "\n",
        "#train it and print performance \n",
        "print(\"Start Training...\")\n",
        "for i in range(filters_tested):\n",
        "  for j in range(3):\n",
        "    records[i][j] = model[i][j].fit(X_train_reduced,y_train_reduced, epochs = epochs, batch_size=batch_size, validation_data = (X_val,y_val), verbose=0)\n",
        "    print (\"Run:\", j+1, \" Two layer CNN with: \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \", format(max(records[i][j].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i][j].history['val_accuracy']), \".5f\"), \" Training Loss: \", format(min(records[i][j].history['loss']), \".5f\"), \" Validation Loss: \", format(min(records[i][j].history['val_loss']), \".5f\"))\n",
        "    averages[i] = list(x + y for (x, y) in zip(averages[i], records[i][j].history['val_accuracy']))\n",
        "\n",
        "\n",
        "#print graphs of performance of different model architecture across epochs \n",
        "for i in range(filters_tested):\n",
        "  averages[i] = [x / 3 for x in averages[i]]\n",
        "  plt.plot(list(range(1,epochs+1,1)), averages[i])\n",
        "print(averages)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuaracy')\n",
        "plt.title('Accuracy of Model Over Epochs')\n",
        "plt.legend(model_names, loc=\"upper left\")\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.97,1])\n",
        "plt.xticks(list(range(1,epochs+1,1)))\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-74496a12b810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_layer_filter_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_layer_filter_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/version_utils.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0muse_v2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshould_use_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswap_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_layer_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_v2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=self-cls-assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayerVersionSelector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/version_utils.py\u001b[0m in \u001b[0;36mswap_class\u001b[0;34m(cls, v2_cls, v1_cls, use_v2)\u001b[0m\n\u001b[1;32m    108\u001b[0m   \u001b[0;31m# Recursively search superclasses to swap in the right Keras class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m   cls.__bases__ = tuple(\n\u001b[0;32m--> 110\u001b[0;31m       swap_class(base, v2_cls, v1_cls, use_v2) for base in cls.__bases__)\n\u001b[0m\u001b[1;32m    111\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0Oplha_TXOr"
      },
      "source": [
        "Based on the graph produced from the code above a model with 32 and 64 filters in its first two layers is ideal, as it perfromance is almost identical to the one with 64 ad 128 filters, but it's architectual simplicity will make training run much quicker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roC0oErfUJ9n"
      },
      "source": [
        "# **Build Deep Neural Network III**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "A0cjzQOgUVL-",
        "outputId": "592739f7-f692-485d-bd78-05f6ca54cac6"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import History\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_rows, image_cols = 28, 28\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "X_train = X_train.reshape(X_train.shape[0], image_rows, image_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_rows, image_cols, 1)\n",
        "batch_size = 100\n",
        "epochs = 15\n",
        "\n",
        "#next we test to see what is the best # of dense nodes to use in the last hidden layer\n",
        "dense_nodes_tested = 5\n",
        "model = [[0] * 3] * dense_nodes_tested\n",
        "model_names = [\"32 dense nodes\", \"64 dense nodes\", \"128 dense nodes\", \"256 dense nodes\", \"512 dense nodes\", \"1024 dense nodes\" ]\n",
        "averages = [[0] * epochs] * dense_nodes_tested\n",
        "\n",
        "#build the different models, 3 of each type so we get an average across each parameter we're testing for\n",
        "for i in range(dense_nodes_tested):\n",
        "  for j in range(3):\n",
        "    model[i][j] = Sequential()\n",
        "    model[i][j].add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
        "    model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model[i][j].add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu', padding='same'))\n",
        "    model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model[i][j].add(Flatten())\n",
        "    model[i][j].add(Dense(2**(5+i), activation='relu'))\n",
        "    model[i][j].add(Dense(10, activation='softmax'))\n",
        "    model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#split data into train and val sets\n",
        "X_val = X_train[0: int(X_train.shape[0]/4)]\n",
        "y_val = y_train[0:int(y_train.shape[0]/4)]\n",
        "X_train_reduced = X_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "y_train_reduced = y_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "records = [[0] * 3] * dense_nodes_tested\n",
        "\n",
        "#train it and print performance \n",
        "for i in range(dense_nodes_tested):\n",
        "  for j in range(3):\n",
        "    records[i][j] = model[i][j].fit(X_train_reduced,y_train_reduced, epochs = epochs, batch_size=batch_size, validation_data = (X_val,y_val), verbose=0)\n",
        "    print(\"Two layer CNN with: \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \", format(max(records[i][j].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i][j].history['val_accuracy']), \".5f\"), \" Training Loss: \", format(min(records[i][j].history['loss']), \".5f\"), \" Validation Loss: \", format(min(records[i][j].history['val_loss']), \".5f\"))\n",
        "    averages[i] = list(x + y for (x, y) in zip(averages[i], records[i][j].history['val_accuracy']))\n",
        "\n",
        "#print graphs of performance of different model architecture across epochs \n",
        "for i in range(dense_nodes_tested):\n",
        "  averages[i] = [x / 3 for x in averages[i]]\n",
        "  plt.plot(list(range(1,epochs+1,1)), averages[i])\n",
        "print(averages)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuaracy')\n",
        "plt.title('Accuracy of Model Over Epochs')\n",
        "plt.legend(model_names, loc=\"upper left\")\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.97,1])\n",
        "plt.xticks(list(range(1,epochs+1,1)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-95906f332317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         dtype=self.dtype)\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                         shape=None):\n\u001b[1;32m    198\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2598\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1516\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1649\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1651\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1653\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/initializers_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    395\u001b[0m        \u001b[0;34m(\u001b[0m\u001b[0mvia\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_floatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVarianceScaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m       \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1042\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     return op(\n\u001b[0;32m-> 1044\u001b[0;31m         shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, minval, maxval, dtype, seed, name)\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       result = gen_random_ops.random_uniform(\n\u001b[0;32m--> 302\u001b[0;31m           shape, dtype, seed=seed1, seed2=seed2)\n\u001b[0m\u001b[1;32m    303\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mminval_is_zero\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmaxval_is_one\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[1;32m    721\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m    722\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"RandomUniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m         tld.op_callbacks, shape, \"seed\", seed, \"seed2\", seed2, \"dtype\", dtype)\n\u001b[0m\u001b[1;32m    724\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3pFHW6NhKgi"
      },
      "source": [
        "Based on the graph produced from the code above, the best performing number of dense nodes in the last hidden layer was 128 as it was just slightly less than 512, but would be much more efficient when trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd3oemb7hR96"
      },
      "source": [
        "# **Build Deep Neural Network IV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "VDSiKNIihWxY",
        "outputId": "ad433852-9a4f-404e-91d0-8bebcb71fe30"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import History\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_rows, image_cols = 28, 28\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "X_train = X_train.reshape(X_train.shape[0], image_rows, image_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_rows, image_cols, 1)\n",
        "batch_size = 100\n",
        "epochs = 15\n",
        "\n",
        "#next we test to see what is the best dropout rate to use for the two layer model\n",
        "dropout_tested = 5\n",
        "model = [[0] * 3] * dropout_tested\n",
        "model_names = [\".20 dropout\", \".25 dropout\", \".30 dropout\", \".35 dropout\", \".40 dropout\"]\n",
        "\n",
        "#build the different models, 3 of each type so we get an average across each parameter we're testing for\n",
        "for i in range(dropout_tested):\n",
        "  for j in range(3):\n",
        "    model[i][j] = Sequential()\n",
        "    model[i][j].add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
        "    model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model[i][j].add(Dropout(rate=(0.05*i + 0.20)))\n",
        "    model[i][j].add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu', padding='same'))\n",
        "    model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "    model[i][j].add(Dropout(rate=(0.05*i + 0.20)))\n",
        "    model[i][j].add(Flatten())\n",
        "    model[i][j].add(Dense(128, activation='relu'))\n",
        "    model[i][j].add(Dropout(rate=(0.05*i + 0.20)))\n",
        "    model[i][j].add(Dense(10, activation='softmax'))\n",
        "    model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#split data into train and val sets\n",
        "X_val = X_train[0: int(X_train.shape[0]/4)]\n",
        "y_val = y_train[0:int(y_train.shape[0]/4)]\n",
        "X_train_reduced = X_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "y_train_reduced = y_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "records = [[0] * 3] * dropout_tested\n",
        "averages = [[0] * epochs] * dropout_tested\n",
        "\n",
        "#train it and print performance \n",
        "for i in range(dropout_tested):\n",
        "  for j in range(3):\n",
        "    records[i][j] = model[i][j].fit(X_train_reduced,y_train_reduced, epochs = epochs, batch_size=batch_size, validation_data = (X_val,y_val), verbose=0)\n",
        "    print(\"Two layer CNN with: \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \", format(max(records[i][j].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i][j].history['val_accuracy']), \".5f\"), \" Training Loss: \", format(min(records[i][j].history['loss']), \".5f\"), \" Validation Loss: \", format(min(records[i][j].history['val_loss']), \".5f\"))\n",
        "    averages[i] = list(x + y for (x, y) in zip(averages[i], records[i][j].history['val_accuracy']))\n",
        "\n",
        "#print graphs of performance of different model architecture across epochs \n",
        "for i in range(dropout_tested):\n",
        "  averages[i] = [x / 3 for x in averages[i]]\n",
        "  plt.plot(list(range(1,epochs+1,1)), averages[i])\n",
        "print(averages)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuaracy')\n",
        "plt.title('Accuracy of Model Over Epochs')\n",
        "plt.legend(model_names, loc=\"upper left\")\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.97,1])\n",
        "plt.xticks(list(range(1,epochs+1,1)))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-264e07d486ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout_tested\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_reduced\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_reduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Two layer CNN with: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Epochs Trained: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Training Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".5f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Validation Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".5f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Training Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".5f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Validation Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".5f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[...\n\u001b[1;32m     58\u001b[0m     \u001b[0maverages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzMRXIldmiR4"
      },
      "source": [
        "The best performing dropout rate based on the above experiment was 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzFSeZLemoDk"
      },
      "source": [
        "# **Build Deep Neural Network V**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kydneHzmoRu"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import History\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_rows, image_cols = 28, 28\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "X_train = X_train.reshape(X_train.shape[0], image_rows, image_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_rows, image_cols, 1)\n",
        "batch_size = 100\n",
        "epochs = 15\n",
        "\n",
        "# next we test to see what is the best extra parameters to use for the two layer model\n",
        "extra_tested = 8\n",
        "model = [[0] * 3] * extra_tested\n",
        "model_names = [\"control\", \"3KS\", \"5F-2S\", \"BN\", \"DA\", \"3KS and 5F-2S\", \"All\", \"All and ES\"]\n",
        "\n",
        "#build the different models, 3 of each type so we get an average across each parameter we're testing for\n",
        "for i in range(extra_tested):\n",
        "  for j in range(3):\n",
        "    if (i == 0):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), activation = 'relu', input_shape = (28, 28,1), padding = 'same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Dropout(rate=(0.05 * i + 0.20)))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (5, 5), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    if (i == 1):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', input_shape = (28, 28,1), padding = 'same'))\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    if (i == 2):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), activation = 'relu', input_shape = (28, 28,1), padding = 'same'))\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (5, 5), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    if (i == 3):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), activation = 'relu', input_shape = (28, 28, 1), padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (5, 5), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    if (i == 4):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), activation = 'relu', input_shape = (28, 28,1), padding = 'same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (5, 5), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(MaxPool2D(pool_size=(2, 2)))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    if (i == 5):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', input_shape = (28, 28,1), padding = 'same'))\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = ( 5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    if (i == 6):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', input_shape = (28, 28,1), padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    if (i == 7):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', input_shape = (28, 28,1), padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#split data into train and val sets\n",
        "X_val = X_train[0: int(X_train.shape[0] / 4)]\n",
        "y_val = y_train[0:int(y_train.shape[0] / 4)]\n",
        "X_train_reduced = X_train[int(X_train.shape[0] / 4):(X_train.shape[0])]\n",
        "y_train_reduced = y_train[int(X_train.shape[0] / 4):(X_train.shape[0])]\n",
        "records = [[0] * 3] * extra_tested\n",
        "averages = [[0] * epochs] * extra_tested\n",
        "datagen = ImageDataGenerator(rotation_range=15, zoom_range=0.15, width_shift_range=0.15, height_shift_range=0.15)\n",
        "\n",
        "#train it and print performance \n",
        "print(\"Start training...\")\n",
        "for i in range(extra_tested):\n",
        "  for j in range(3):\n",
        "    if (i == 4 or i == 6):  # data augmentation\n",
        "      records[i][j] = model[i][j].fit_generator(datagen.flow(X_train_reduced, y_train_reduced, batch_size=batch_size), epochs=epochs, validation_data=(X_val, y_val))\n",
        "      print(\"Run: \", j + 1, \" Two layer CNN with: \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \",\n",
        "            format(max(records[i][j].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i][j].history['val_accuracy']), \".5f\"),\n",
        "            \" Training Loss: \", format(min(records[i][j].history['loss']), \".5f\"), \" Validation Loss: \",\n",
        "            format(min(records[i][j].history['val_loss']), \".5f\"))\n",
        "      averages[i] = list(x + y for (x, y) in zip(averages[i], records[i][j].history['val_accuracy']))\n",
        "\n",
        "    elif (i == 7):  # data augmentation and early stopping\n",
        "      callbacks = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=0, verbose=1, mode='min', baseline=None,\n",
        "                                restore_best_weights=True)\n",
        "      records[i][j] = model[i][j].fit_generator(datagen.flow(X_train_reduced, y_train_reduced, batch_size=batch_size),\n",
        "                                          epochs=epochs, validation_data=(X_val, y_val))\n",
        "      print(\"Two layer CNN with: \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \",\n",
        "            format(max(records[i][j].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i][j].history['val_accuracy']), \".5f\"),\n",
        "            \" Training Loss: \", format(min(records[i][j].history['loss']), \".5f\"), \" Validation Loss: \",\n",
        "            format(min(records[i][j].history['val_loss']), \".5f\"))\n",
        "      averages[i] = list(x + y for (x, y) in zip(averages[i], records[i][j].history['val_accuracy']))\n",
        "\n",
        "    else:\n",
        "      records[i][j] = model[i][j].fit(X_train_reduced, y_train_reduced, epochs=epochs, batch_size=batch_size,\n",
        "                                validation_data=(X_val, y_val), verbose=0)\n",
        "      print(\"Two layer CNN with: \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \",\n",
        "            format(max(records[i][j].history['accuracy']), \".5f\"), \" Validation Accuracy: \",\n",
        "            format(max(records[i][j].history['val_accuracy']), \".5f\"), \" Training Loss: \",\n",
        "            format(min(records[i][j].history['loss']), \".5f\"), \" Validation Loss: \",\n",
        "            format(min(records[i][j].history['val_loss']), \".5f\"))\n",
        "      averages[i] = list(x + y for (x, y) in zip(averages[i], records[i][j].history['val_accuracy']))\n",
        "\n",
        "#print graphs of performance of different model architecture across epochs \n",
        "for i in range(extra_tested):\n",
        "  averages[i] = [x / 3 for x in averages[i]]\n",
        "  plt.plot(list(range(1,epochs+1,1)), averages[i])\n",
        "print(averages)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuaracy')\n",
        "plt.title('Accuracy of Model Over Epochs')\n",
        "plt.legend(model_names, loc=\"upper left\")\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.97,1])\n",
        "plt.xticks(list(range(1,epochs+1,1)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpK6t0O1RCNA"
      },
      "source": [
        "The final model we choose is one with early stopping and the following architecture:  two 3 kernel layers in a row instead of one 5 kernel filter layer, with a 5 kernel filter with stride two instead of max pooling, with batch normalizationa and data augmentation. All we have left is to determine the best data augmentation hyper parameter to choose, the number of CNN's to train in parallel, and the best batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIwZfD4YRmPZ"
      },
      "source": [
        "# **Build Deep Neural Network VI**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMBmWxXKRtYV"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import History\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_rows, image_cols = 28, 28\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "X_train = X_train.reshape(X_train.shape[0], image_rows, image_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_rows, image_cols, 1)\n",
        "\n",
        "#next we test to see what is the best data augmentation parameters for our model are\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "DA_tested = 9\n",
        "model = [[0] * 3] * DA_tested\n",
        "model_names = [\"rr10-0.10\", \"rr10-0.15\", \"rr10-0.20\", \"rr15-0.10\", \"rr15-0.15\", \"rr15-0.20\", \"rr20-0.10\", \"rr20-0.15\", \"rr20-0.20\"]\n",
        "datagens = [0] * DA_tested\n",
        "rr = 0\n",
        "other = 0\n",
        "i = 0\n",
        "\n",
        "#build the different models, 3 of each type so we get an average across each parameter we're testing for\n",
        "while(i < DA_tested):\n",
        "  for j in range(3):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', input_shape = (28, 28,1), padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  if (i < 3):\n",
        "    rot_range = 10\n",
        "    if (i == 0):\n",
        "        other = 0.1\n",
        "        datagens[i] = ImageDataGenerator(rotation_range=rr, zoom_range=other, width_shift_range=other,height_shift_range=other)\n",
        "    elif (i == 1):\n",
        "        other = 0.15\n",
        "        datagens[i] = ImageDataGenerator(rotation_range=rr, zoom_range=other, width_shift_range=other,height_shift_range=other)\n",
        "    else:\n",
        "        other = 0.2\n",
        "        datagens[i] = ImageDataGenerator(rotation_range=rr, zoom_range=other, width_shift_range=other,height_shift_range=other)\n",
        "\n",
        "  if (i < 6 and i > 2):\n",
        "    rot_range = 15\n",
        "    if (i == 3):\n",
        "        other = 0.1\n",
        "        datagens[i] = ImageDataGenerator(rotation_range=rr, zoom_range=other, width_shift_range=other,height_shift_range=other)\n",
        "    elif (i == 4):\n",
        "        other = 0.15\n",
        "        datagens[i] = ImageDataGenerator(rotation_range=rr, zoom_range=other, width_shift_range=other,height_shift_range=other)\n",
        "    else:\n",
        "        other = 0.2\n",
        "        datagens[i] = ImageDataGenerator(rotation_range=rr, zoom_range=other, width_shift_range=other,height_shift_range=other)\n",
        "\n",
        "  if (i < 9 and i > 5):\n",
        "    rot_range = 20\n",
        "    if (i == 6):\n",
        "        other = 0.1\n",
        "        datagens[i] = ImageDataGenerator(rotation_range=rr, zoom_range=other, width_shift_range=other,height_shift_range=other)\n",
        "    elif (i == 7):\n",
        "        other = 0.15\n",
        "        datagens[i] = ImageDataGenerator(rotation_range=rr, zoom_range=other, width_shift_range=other,height_shift_range=other)\n",
        "    else:\n",
        "        other = 0.2\n",
        "        datagens[i] = ImageDataGenerator(rotation_range=rr, zoom_range=other, width_shift_range=other,height_shift_range=other)\n",
        "  i+=1\n",
        "\n",
        "#split data into train and val sets\n",
        "X_val = X_train[0: int(X_train.shape[0]/4)]\n",
        "y_val = y_train[0:int(y_train.shape[0]/4)]\n",
        "X_train_reduced = X_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "y_train_reduced = y_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "records = [[0] * 3] * DA_tested\n",
        "averages = [[0] * epochs] * DA_tested\n",
        "\n",
        "#train it and print performance \n",
        "for i in range(DA_tested):\n",
        "  for j in range(3):\n",
        "    records[i][j] = model[i][j].fit_generator(datagens[i].flow(X_train_reduced, y_train_reduced, batch_size=batch_size), epochs=epochs, validation_data=(X_val, y_val))\n",
        "    print(\"Run: \", j + 1, \" Two layer CNN with: \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \", format(max(records[i][j].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i][j].history['val_accuracy']), \".5f\"), \" Training Loss: \", format(min(records[i][j].history['loss']), \".5f\"), \" Validation Loss: \", format(min(records[i][j].history['val_loss']), \".5f\"))\n",
        "    averages[i] = list(x + y for (x, y) in zip(averages[i], records[i][j].history['val_accuracy']))\n",
        "\n",
        "#print graphs of performance of different model architecture across epochs \n",
        "for i in range(DA_tested):\n",
        "  averages[i] = [x / 3 for x in averages[i]]\n",
        "  plt.plot(list(range(1,epochs+1,1)), averages[i])\n",
        "print(averages)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuaracy')\n",
        "plt.title('Accuracy of Model Over Epochs')\n",
        "plt.legend(model_names, loc=\"upper left\")\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.97,1])\n",
        "plt.xticks(list(range(1,epochs+1,1)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeFv-k_9Z0xd"
      },
      "source": [
        "Based on the graph produced from the code above, the best data augmentation setup is the one with rotation range 15 and zoom range, width shift, and height shift 0.15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLsPPsqlZ7RS"
      },
      "source": [
        "# **Build Deep Neural Network VII**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "lvx_aiH9aBSF",
        "outputId": "ba21fa8e-3508-47cc-80f0-856e43e5884a"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import History\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_rows, image_cols = 28, 28\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "y_test = np.asarray(y_test, dtype=int)\n",
        "X_train = X_train.reshape(X_train.shape[0], image_rows, image_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_rows, image_cols, 1)\n",
        "\n",
        "#next we test to see what the best # of cnn's to average togther the predictions of is\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "models_tested = 20\n",
        "model = [0] * models_tested\n",
        "model_names = list(range(1, models_tested + 1, 1))\n",
        "predictions = np.zeros((X_test.shape[0], 10))\n",
        "accuracy = [0] * models_tested\n",
        "records = [0] * models_tested\n",
        "datagen = ImageDataGenerator(rotation_range=15, zoom_range=0.15, width_shift_range=0.15,height_shift_range=0.15)\n",
        "\n",
        "#build the different models, 3 of each type so we get an average across each parameter we're testing for\n",
        "for i in range(models_tested):\n",
        "  model[i] = Sequential()\n",
        "  model[i].add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Conv2D(filters=32, kernel_size=(5, 5), strides=2, activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Dropout(rate=0.3))\n",
        "  model[i].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Conv2D(filters=64, kernel_size=(5, 5), strides=2, activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Dropout(rate=0.3))\n",
        "  model[i].add(Flatten())\n",
        "  model[i].add(Dense(128, activation='relu'))\n",
        "  model[i].add(Dropout(rate=0.3))\n",
        "  model[i].add(Dense(10, activation='softmax'))\n",
        "  model[i].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#we set up the model architecture, now we have to train them\n",
        "for i in range(models_tested):\n",
        "    #split data into train and val sets before running each model\n",
        "    X_val = X_train[0: int(X_train.shape[0] / 4)]\n",
        "    y_val = y_train[0:int(y_train.shape[0] / 4)]\n",
        "    X_train_reduced = X_train[int(X_train.shape[0] / 4):(X_train.shape[0])]\n",
        "    y_train_reduced = y_train[int(X_train.shape[0] / 4):(X_train.shape[0])]\n",
        "\n",
        "    records[i]= model[i].fit_generator(datagen.flow(X_train_reduced, y_train_reduced, batch_size=batch_size),\n",
        "                                              epochs=epochs, validation_data=(X_val, y_val))\n",
        "    print(\"CNN \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \",\n",
        "          format(max(records[i].history['accuracy']), \".5f\"), \" Validation Accuracy: \",\n",
        "          format(max(records[i].history['val_accuracy']), \".5f\"), \" Training Loss: \",\n",
        "          format(min(records[i].history['loss']), \".5f\"), \" Validation Loss: \",\n",
        "          format(min(records[i].history['val_loss']), \".5f\"))\n",
        "\n",
        "# sum predicted outputs across each of the individual models, then take max of the prediction\n",
        "for i in range(models_tested):\n",
        "  predictions += model[i].predict(X_test)\n",
        "  correct = 0.0\n",
        "  for j in range(y_test.shape[0]):\n",
        "    if (np.argmax(y_test[j]) == np.argmax(predictions[j])):\n",
        "      correct += 1\n",
        "  accuracy[i] = correct / X_test.shape[0]\n",
        "  print(i+1, \" CNN's had an average test accuracy of \", accuracy[i])\n",
        "\n",
        "#print graphs of performance of different # of models\n",
        "plt.plot(list(range(1, models_tested + 1, 1)), accuracy)\n",
        "plt.xlabel(\"Numer of CNN's Averaged Across\")\n",
        "plt.ylabel('Test Accuaracy')\n",
        "plt.title('Test Accuracy of Multiple Models')\n",
        "plt.legend(model_names, loc=\"upper left\")\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.97, 1])\n",
        "plt.xticks(list(range(1, models_tested + 1, 1)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-fab406c8aa58>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    image_rows, image_cols = 28, 28\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn53-feD3veB"
      },
      "source": [
        "The optimal number of CNN's to average together is 10, as they perfromed similarly in accuracy as 15-20 models, but would require a lot less computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-G-aDlVKXNTc"
      },
      "source": [
        "# **Build Deep Neural Network VIII**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66z7c8QEXOGG"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import History\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_rows, image_cols = 28, 28\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "X_train = X_train.reshape(X_train.shape[0], image_rows, image_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_rows, image_cols, 1)\n",
        "\n",
        "#next we test to see what the optimal batch size is\n",
        "batch_size = 100\n",
        "epochs = 10\n",
        "batches_tested = 5\n",
        "model = [[0] * 3] * batches_tested\n",
        "model_names = [\"16 BS\", \"32 BS\", \"64 BS\", \"128 BS\", \"256 BS\"]\n",
        "batches = [0] * batches_tested\n",
        "\n",
        "#build the different models, 3 of each type so we get an average across each parameter we're testing for\n",
        "for i in range(batches_tested):\n",
        "  for j in range(3):\n",
        "      model[i][j] = Sequential()\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', input_shape = (28, 28,1), padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=32, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Conv2D(filters=64, kernel_size = (5, 5), strides = 2, activation = 'relu', padding = 'same'))\n",
        "      model[i][j].add(BatchNormalization())\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Flatten())\n",
        "      model[i][j].add(Dense(128, activation = 'relu'))\n",
        "      model[i][j].add(Dropout(rate=0.3))\n",
        "      model[i][j].add(Dense(10, activation='softmax'))\n",
        "      model[i][j].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#split data into train and val sets\n",
        "X_val = X_train[0: int(X_train.shape[0]/4)]\n",
        "y_val = y_train[0:int(y_train.shape[0]/4)]\n",
        "X_train_reduced = X_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "y_train_reduced = y_train[int(X_train.shape[0]/4):(X_train.shape[0])]\n",
        "records = [[0] * 3] * batches_tested\n",
        "averages = [[0] * epochs] * batches_tested\n",
        "datagen = ImageDataGenerator(rotation_range=15, zoom_range=0.15, width_shift_range=0.15,height_shift_range=0.15)\n",
        "\n",
        "#train it and print performance \n",
        "for i in range(batches_tested):\n",
        "  for j in range(3):\n",
        "    records[i][j] = model[i][j].fit_generator(datagen.flow(X_train_reduced, y_train_reduced, batch_size=(16*(i+1))), epochs=epochs, validation_data=(X_val, y_val))\n",
        "    print(\"Run: \", j + 1, \" Two layer CNN with: \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \", format(max(records[i][j].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i][j].history['val_accuracy']), \".5f\"), \" Training Loss: \", format(min(records[i][j].history['loss']), \".5f\"), \" Validation Loss: \", format(min(records[i][j].history['val_loss']), \".5f\"))\n",
        "    averages[i] = list(x + y for (x, y) in zip(averages[i], records[i][j].history['val_accuracy']))\n",
        "\n",
        "#print graphs of performance of different model batch sizes across epochs \n",
        "for i in range(batches_tested):\n",
        "  averages[i] = [x / 3 for x in averages[i]]\n",
        "  plt.plot(list(range(1,epochs+1,1)), averages[i])\n",
        "print(averages)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuaracy')\n",
        "plt.title('Accuracy of Model Over Epochs')\n",
        "plt.legend(model_names, loc=\"upper left\")\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0.97,1])\n",
        "plt.xticks(list(range(1,epochs+1,1)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMPrAAzKYD_t"
      },
      "source": [
        "The ideal batch size is 126 as it performed the best on the test above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFrOem6x30Hm"
      },
      "source": [
        "# **Build Deep Neural Network IX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INRl-BIo3zOF",
        "outputId": "d6d9194b-5b80-4bcc-ed59-ecfac26e3ac1"
      },
      "source": [
        "#train and test the final model\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential, model_from_json\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Flatten, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import History\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "#set up final model parameters\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "image_rows, image_cols = 28, 28\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "y_test = np.asarray(y_test, dtype=int)\n",
        "X_train = X_train.reshape(X_train.shape[0], image_rows, image_cols, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], image_rows, image_cols, 1)\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "num_models = 15\n",
        "records = [0] * num_models\n",
        "model_names = [\"model1\", \"model2\", \"model3\", \"model4\", \"model5\", \"model6\", \"model7\", \"model8\", \"model9\", \"model10\", \"model11\", \"model12\", \"model13\", \"model14\", \"model15\"]\n",
        "callbacks = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=0, verbose=1, mode='min', baseline=None, restore_best_weights=True) ##MIGHT USE depends on earlier test\n",
        "datagen = ImageDataGenerator(rotation_range=15, zoom_range=0.15, width_shift_range=0.15,height_shift_range=0.15)\n",
        "predictions = np.zeros((X_test.shape[0],10))\n",
        "accuracy = [0] * models_tested\n",
        "\n",
        "#build the different number of models we're going to average across\n",
        "for i in range(num_models):\n",
        "  model[i] = Sequential()\n",
        "  model[i].add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1), padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Conv2D(filters=32, kernel_size=(5, 5), strides=2, activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Dropout(rate=0.3))\n",
        "  model[i].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Conv2D(filters=64, kernel_size=(5, 5), strides=2, activation='relu', padding='same'))\n",
        "  model[i].add(BatchNormalization())\n",
        "  model[i].add(Dropout(rate=0.3))\n",
        "  model[i].add(Flatten())\n",
        "  model[i].add(Dense(128, activation='relu'))\n",
        "  model[i].add(Dropout(rate=0.3))\n",
        "  model[i].add(Dense(10, activation='softmax'))\n",
        "  model[i].compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#train/save models and print performance \n",
        "for i in range(num_models):\n",
        "  #split data into train and val sets for each model\n",
        "  X_val = X_train[0: int(X_train.shape[0]/4)]\n",
        "  y_val = y_train[0:int(y_train.shape[0]/4)]\n",
        "  X_train_reduced = X_train[int(X_train.shape[0]/4):(X_train.shape[0])] \n",
        "  y_train_reduced = y_train[int(X_train.shape[0]/4):(X_train.shape[0])] \n",
        "  records = [0] * models_tested\n",
        "  \n",
        "  records[i] = model[i].fit_generator(datagen.flow(X_train_reduced, y_train_reduced, batch_size=batch_size), epochs=epochs, validation_data=(X_val, y_val))\n",
        "  print(\" CNN \", model_names[i], \" Epochs Trained: \", epochs, \" Training Accuracy: \", format(max(records[i].history['accuracy']), \".5f\"), \" Validation Accuracy: \", format(max(records[i].history['val_accuracy']), \".5f\"), \" Training Loss: \", format(min(records[i].history['loss']), \".5f\"), \" Validation Loss: \", format(min(records[i].history['val_loss']), \".5f\"))\n",
        "  model[i].save(model_names[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "432.46\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ffz2V179OhO"
      },
      "source": [
        "# **Generate CSV File Based on Saved Trained Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk_7lKeb9fM-"
      },
      "source": [
        "import keras\r\n",
        "import numpy as np\r\n",
        "import csv\r\n",
        "\r\n",
        "model_names = [\"model1\", \"model2\", \"model3\", \"model4\", \"model5\", \"model6\", \"model7\", \"model8\", \"model9\", \"model10\", \"model11\", \"model12\", \"model13\", \"model14\", \"model15\"]\r\n",
        "num_models = 15\r\n",
        "model = [0] * num_models\r\n",
        "PATH = \"/content/gdrive/My Drive/files-forp3/andrei_models3/\" \r\n",
        "\r\n",
        "# load saved models\r\n",
        "for i in range(num_models):\r\n",
        "    model[i] = keras.models.load_model(PATH + model_names[i])\r\n",
        "\r\n",
        "# create csv file\r\n",
        "with open('cheese3.csv', 'w', newline='') as file:\r\n",
        "    writer = csv.writer(file)\r\n",
        "    writer.writerow([\"Id\", \"Label\"])\r\n",
        "\r\n",
        "    for i in range(len(main)):\r\n",
        "        image = main[i]\r\n",
        "        image = np.array(image)\r\n",
        "\r\n",
        "        # get best prediction across all models\r\n",
        "        current = np.zeros((len(image), 10))\r\n",
        "        for j in range(num_models):\r\n",
        "            current += np.array(model[j](image,training=False))\r\n",
        "\r\n",
        "        # create strings for each label\r\n",
        "        curr = \"\"\r\n",
        "        for k in range(current.shape[0]):\r\n",
        "            curr += str(np.argmax(current[k]))\r\n",
        "\r\n",
        "        # append missing 10's\r\n",
        "        for m in range(5 - image.shape[0]):\r\n",
        "            curr += \"10\"\r\n",
        "\r\n",
        "        writer.writerow([i, curr])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}